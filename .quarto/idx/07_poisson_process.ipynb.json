{"title":"泊松过程","markdown":{"yaml":{"title":"泊松过程"},"headingText":"Gamma 分布","containsRefs":false,"markdown":"\n\n\n\n\n\n\n\n首先，我建议大家把我在博客讲过的 [离散分布](https://hongtaoh.com/cn/2024/03/23/discrete-distributions/)先理解透，特别是泊松分布那一部分。\n\n在泊松分布中我们讲到这样一个问题：\n\n>中国男足队平均每场比赛进 5 个球 (请允许我在平行宇宙做一次梦)，请问下一场比赛中国进 2 个球的概率是多少\n\n我们用泊松分布模拟了中国队进 0-10 个球的概率：\n\n$$\nP(X = k) = \\frac{\\lambda^k \\cdot e^{-\\lambda}}{k!}\n$$ {#eq-poisson}\n\n使用泊松分布，我们可以由 $\\lambda$ 得知 $p(k)$，也就是 $P(k|\\lambda)$。那现在的问题是，我们如何求 $P(\\lambda | k)$:\n\n>中国队在一场球赛中进了 2 个球 ($k=2$)，请问中国队平常一般进多少个球 ($\\lambda$)？\n\n根据贝叶斯定理：\n\n$$\nP(\\lambda | k) = \\frac{P(\\lambda) P(k|\\lambda)}{P(k)}\n$$ {#eq-lambda-given-k}\n\n这里\n\n- $k$ 是数据\n- $\\lambda$ 是假设\n- $P(\\lambda)$ 是先验概率\n- $P(k|\\lambda)$ 是似然\n\n我们可以先忽略 $P(k)$，因为它只是为了让最后的结果之和为 1。\n\n::: {.callout-tip}\n\n为什么我们可以忽略 $P(k)$ 呢？因为它是一个常量：\n\n$$\nP(k) = \\sum_{\\lambda} P(k|\\lambda) \\cdot P(\\lambda)\n$$\n\n我们最后让 posteior 和为一的时候，也是要对所有的结果乘上一个常量。那我们最后再做就好，没有必要这时候乘。\n\n:::\n\n$P(k|\\lambda)$ 可以通过泊松分布来求得。现在比较难弄的是 $P(\\lambda)$，也就是我们还没看任何数据时对球队一般进球的预估。\n\n我们用 [FIFA 的数据](https://www.statista.com/statistics/269031/goals-scored-per-game-at-the-fifa-world-cup-since-1930/) (世界杯场均进分数据)：\n\n我们看到一场球平均进 3 个球。那我们大概就有数了。但是 Prior 不能只是一个数，需要是一个分布，我们用 [Gamma 分布](https://en.wikipedia.org/wiki/Gamma_distribution)来模拟。该分布有两个变量：$\\alpha$ (shape) 和 $\\beta$ (rate)。该分布的平均值为 $\\frac{\\alpha}{\\beta}$，Probability Density Function (PDF) 是\n\n$$\nf(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-\\beta x} \n$$ {#eq-gamma}\n\n其中\n\n$$\nx \\in (0, \\infty)\n$$\n\n其中 $\\Gamma(\\alpha)$ 的定义是：\n\n$$\n\\Gamma(\\alpha) = \\int_0^{\\infty} x^{\\alpha - 1} e^{-x} \\, dx\n$$\n\n$\\Gamma(\\alpha)$ 其实是连乘 (factorial) 在非整数中的应用。如果 $n$ 是整数，那\n\n$$\\Gamma(n) = (n - 1)!$$\n\n如果是非整数，比如 $0.5$，那就需要用上面的那个普遍公式。\n\n为什么用 Gamma 分布？\n\n- 因为它的自变量是从 0 开始的。\n- 如果我们把 $\\beta$ 预设为 1，那么就只剩下一个变量 $\\alpha$，这个就是上面算出来的 3 个球。\n- 结果符合现实情况。\n\n所以以下就是 Prior:\n\n::: {.callout-tip}\n\n注意，`gamma(alpha_prior).pdf(lambdas)` 算的是 PDF，要乘以间隔才是 PMF (probability mass function)。\n\n:::\n\nLikelihood，也就是已知 $\\lambda$ 求 $k$:\n\n那求 Posterior 就是 Prior 和 Likelihood 这两个数列逐元素相乘，然后让最后的结果之和为1就可以：\n\n## Probability of Superiority\n\n>德国男足最近的一场比赛射进 4 个球，巴西队最近一场比赛射进 2 个球。我们有多大信心说德国男足更强？\n\n我们首先要把各自队的 posterior 求出来：\n\n这里 $\\lambda$ 所代表的是一个国家队平均而言每场得多少分。\n\n总体思路是这样：德国队随机一个 $\\lambda$，巴西队也随机选一个，德国对应的 $\\lambda$ 比巴西的大的概率。\n\n算法如下：\n\n$$\nP(\\lambda_{\\text{Germany}} > \\lambda_{\\text{Brazil}}) = \\sum_{\\lambda_{\\text{Germany}}} \\sum_{\\lambda_{\\text{Brazil}}} P(\\lambda_{\\text{Germany}}) \\cdot P(\\lambda_{\\text{Brazil}}) \\cdot \\mathbb{I}(\\lambda_{\\text{Germany}} > \\lambda_{\\text{Brazil}})\n$$\n\n其中 $\\mathbb{I}(\\lambda_{\\text{Germany}} > \\lambda_{\\text{Brazil}})$ 是 Indicator function，如果 $\\lambda_{\\text{Germany}} > \\lambda_{\\text{Brazil}}$ 则 $\\mathbb{I}(\\lambda_{\\text{Germany}} > \\lambda_{\\text{Brazil}}) = 1$，反之则为 0。\n\n\n::: {.callout-tip}\n\n想一下下面这个想法为什么是错的：\n\n```py\nsum = 0\nfor i, lam in enumerate(lambdas):\n    if ge_posterior_pmf[i] > br_posterior_pmf[i]:\n        sum += 1\n```\n\n:::\n\n## Conjugate Priors\n\n我们来讲一个很重要的概念：[Conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior)，中文翻译成「共轭先验」。其所表达的意思是：Posterior Distribution 和 Prior Distribution 同属于一个 Distribution。因为我们的 prior 是 gamma distribution，那 posterior 也是，只是 shape (alpha) 和 rate (beta) 不同。\n\n我们具体来看一下。\n\n在公式 @eq-lambda-given-k 中，求先验概率 $P(\\lambda)$ 我们需要用到 @eq-gamma，只需要把 $x$ 换成 $\\lambda$ 就可以。求似然 $P(k|\\lambda)$ 我们用到 @eq-poisson。\n\n那 $P(\\lambda) P(k|\\lambda)$ 就是代入具体的 $\\lambda$，把 @eq-gamma 和 @eq-poisson 相乘即可。\n\n相乘的结果是：\n\n$$\n\\frac{\\lambda^k \\cdot e^{-\\lambda}}{k!} \\cdot \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha) k!} \\cdot \\lambda^{(\\alpha + k) - 1} e^{-(\\beta + 1) \\lambda}\n$$ {#eq-gamma-times-poisson}\n\n$\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}$ 是一个常数，$k!$ 也是一个常数。\n\n有这样一个知识：对于一个 Gamma 函数 $\\Gamma(\\alpha, \\frac{1}{\\beta})$，曲线下的面积（即分布的积分）为 1。 如果乘上一个常数，比如 $2 \\cdot \\Gamma(\\alpha, \\frac{1}{\\beta})$，该分布的形状保持不变，但曲线下的面积不再是 1，而是变为该常数的值。\n\n那我们就可以理解，在 Gamma 分布中，$\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}$ 的作用就是让积分为 1。\n\n为了得到 Posterior distribution, 我们要把所有的 $\\lambda$ 代入 @eq-gamma-times-poisson，得到一串数字，最后我们要确保它们的和为 1。因此，虽然公式 @eq-gamma-times-poisson 中 \\frac{\\beta^\\alpha}{\\Gamma(\\alpha) k!} 和 Gamma 函数的不一样，但这个不重要，最后确保和为 1 的时候，它自动就变成了它该有的样子。\n\n因此我们就可以这么说：Posterior Distribution 也是一个 Gamma Distribution，其参数现在变成了 $\\alpha + k$ 和 $\\beta + 1$：\n\n$$\\lambda | k \\sim \\text{Gamma}(\\alpha + k, \\beta + 1)$$\n\n代码：\n\n我们看到这个结果和我们通过常规的 Prior 与 Likelihood 相乘所得到的结果是一样的。\n\n## 后验预测分布\n\n我们来讲一下 Posterior predictive distribution (后验预测分布)。\n\n我们看到 Posterior distribution 中，X 轴是 $\\lambda$，其含义是平均而言。比如说，$\\lambda = 5$ 代表的意思是这个球队如果比赛无数次，平均下来每场会进 5 个球。\n\n回到刚才的德国和巴西男足的叙事，我们现在如果问：\n\n>两队现在马上要进行一场比赛，请问德国对获胜的概率是多少？\n\n请注意，我们现在已经不再是问 $\\lambda$，而是问具体的一场比赛中每队的进球数，也就是 $k$。\n\n还记得我们上面提到的吗？\n\n$$\nP(k) = \\sum_{\\lambda} P(k|\\lambda) \\cdot P(\\lambda)\n$$\n\n这里 $P(\\lambda)$ 是 posterior distribution，$P(k|\\lambda)$ 是泊松分布。\n\n下面我们计算一下在下一场比赛中德国队获胜的概率。\n\n总体思路是这样：德国队随机一个 $k$，巴西队也随机选一个，德国对应的 $k$ 比巴西的大的概率。\n\n算法如下：\n\n$$\nP(k_{\\text{Germany}} > k_{\\text{Brazil}}) = \\sum_{k} P(k_{\\text{Germany}}) \\cdot P(k_{\\text{Brazil}}) \\cdot \\mathbb{I}(k_{\\text{Germany}} > k_{\\text{Brazil}})\n$$\n","srcMarkdownNoYaml":"\n\n\n\n\n## Gamma 分布\n\n\n\n首先，我建议大家把我在博客讲过的 [离散分布](https://hongtaoh.com/cn/2024/03/23/discrete-distributions/)先理解透，特别是泊松分布那一部分。\n\n在泊松分布中我们讲到这样一个问题：\n\n>中国男足队平均每场比赛进 5 个球 (请允许我在平行宇宙做一次梦)，请问下一场比赛中国进 2 个球的概率是多少\n\n我们用泊松分布模拟了中国队进 0-10 个球的概率：\n\n$$\nP(X = k) = \\frac{\\lambda^k \\cdot e^{-\\lambda}}{k!}\n$$ {#eq-poisson}\n\n使用泊松分布，我们可以由 $\\lambda$ 得知 $p(k)$，也就是 $P(k|\\lambda)$。那现在的问题是，我们如何求 $P(\\lambda | k)$:\n\n>中国队在一场球赛中进了 2 个球 ($k=2$)，请问中国队平常一般进多少个球 ($\\lambda$)？\n\n根据贝叶斯定理：\n\n$$\nP(\\lambda | k) = \\frac{P(\\lambda) P(k|\\lambda)}{P(k)}\n$$ {#eq-lambda-given-k}\n\n这里\n\n- $k$ 是数据\n- $\\lambda$ 是假设\n- $P(\\lambda)$ 是先验概率\n- $P(k|\\lambda)$ 是似然\n\n我们可以先忽略 $P(k)$，因为它只是为了让最后的结果之和为 1。\n\n::: {.callout-tip}\n\n为什么我们可以忽略 $P(k)$ 呢？因为它是一个常量：\n\n$$\nP(k) = \\sum_{\\lambda} P(k|\\lambda) \\cdot P(\\lambda)\n$$\n\n我们最后让 posteior 和为一的时候，也是要对所有的结果乘上一个常量。那我们最后再做就好，没有必要这时候乘。\n\n:::\n\n$P(k|\\lambda)$ 可以通过泊松分布来求得。现在比较难弄的是 $P(\\lambda)$，也就是我们还没看任何数据时对球队一般进球的预估。\n\n我们用 [FIFA 的数据](https://www.statista.com/statistics/269031/goals-scored-per-game-at-the-fifa-world-cup-since-1930/) (世界杯场均进分数据)：\n\n我们看到一场球平均进 3 个球。那我们大概就有数了。但是 Prior 不能只是一个数，需要是一个分布，我们用 [Gamma 分布](https://en.wikipedia.org/wiki/Gamma_distribution)来模拟。该分布有两个变量：$\\alpha$ (shape) 和 $\\beta$ (rate)。该分布的平均值为 $\\frac{\\alpha}{\\beta}$，Probability Density Function (PDF) 是\n\n$$\nf(x) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-\\beta x} \n$$ {#eq-gamma}\n\n其中\n\n$$\nx \\in (0, \\infty)\n$$\n\n其中 $\\Gamma(\\alpha)$ 的定义是：\n\n$$\n\\Gamma(\\alpha) = \\int_0^{\\infty} x^{\\alpha - 1} e^{-x} \\, dx\n$$\n\n$\\Gamma(\\alpha)$ 其实是连乘 (factorial) 在非整数中的应用。如果 $n$ 是整数，那\n\n$$\\Gamma(n) = (n - 1)!$$\n\n如果是非整数，比如 $0.5$，那就需要用上面的那个普遍公式。\n\n为什么用 Gamma 分布？\n\n- 因为它的自变量是从 0 开始的。\n- 如果我们把 $\\beta$ 预设为 1，那么就只剩下一个变量 $\\alpha$，这个就是上面算出来的 3 个球。\n- 结果符合现实情况。\n\n所以以下就是 Prior:\n\n::: {.callout-tip}\n\n注意，`gamma(alpha_prior).pdf(lambdas)` 算的是 PDF，要乘以间隔才是 PMF (probability mass function)。\n\n:::\n\nLikelihood，也就是已知 $\\lambda$ 求 $k$:\n\n那求 Posterior 就是 Prior 和 Likelihood 这两个数列逐元素相乘，然后让最后的结果之和为1就可以：\n\n## Probability of Superiority\n\n>德国男足最近的一场比赛射进 4 个球，巴西队最近一场比赛射进 2 个球。我们有多大信心说德国男足更强？\n\n我们首先要把各自队的 posterior 求出来：\n\n这里 $\\lambda$ 所代表的是一个国家队平均而言每场得多少分。\n\n总体思路是这样：德国队随机一个 $\\lambda$，巴西队也随机选一个，德国对应的 $\\lambda$ 比巴西的大的概率。\n\n算法如下：\n\n$$\nP(\\lambda_{\\text{Germany}} > \\lambda_{\\text{Brazil}}) = \\sum_{\\lambda_{\\text{Germany}}} \\sum_{\\lambda_{\\text{Brazil}}} P(\\lambda_{\\text{Germany}}) \\cdot P(\\lambda_{\\text{Brazil}}) \\cdot \\mathbb{I}(\\lambda_{\\text{Germany}} > \\lambda_{\\text{Brazil}})\n$$\n\n其中 $\\mathbb{I}(\\lambda_{\\text{Germany}} > \\lambda_{\\text{Brazil}})$ 是 Indicator function，如果 $\\lambda_{\\text{Germany}} > \\lambda_{\\text{Brazil}}$ 则 $\\mathbb{I}(\\lambda_{\\text{Germany}} > \\lambda_{\\text{Brazil}}) = 1$，反之则为 0。\n\n\n::: {.callout-tip}\n\n想一下下面这个想法为什么是错的：\n\n```py\nsum = 0\nfor i, lam in enumerate(lambdas):\n    if ge_posterior_pmf[i] > br_posterior_pmf[i]:\n        sum += 1\n```\n\n:::\n\n## Conjugate Priors\n\n我们来讲一个很重要的概念：[Conjugate priors](https://en.wikipedia.org/wiki/Conjugate_prior)，中文翻译成「共轭先验」。其所表达的意思是：Posterior Distribution 和 Prior Distribution 同属于一个 Distribution。因为我们的 prior 是 gamma distribution，那 posterior 也是，只是 shape (alpha) 和 rate (beta) 不同。\n\n我们具体来看一下。\n\n在公式 @eq-lambda-given-k 中，求先验概率 $P(\\lambda)$ 我们需要用到 @eq-gamma，只需要把 $x$ 换成 $\\lambda$ 就可以。求似然 $P(k|\\lambda)$ 我们用到 @eq-poisson。\n\n那 $P(\\lambda) P(k|\\lambda)$ 就是代入具体的 $\\lambda$，把 @eq-gamma 和 @eq-poisson 相乘即可。\n\n相乘的结果是：\n\n$$\n\\frac{\\lambda^k \\cdot e^{-\\lambda}}{k!} \\cdot \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\lambda^{\\alpha - 1} e^{-\\beta \\lambda} = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha) k!} \\cdot \\lambda^{(\\alpha + k) - 1} e^{-(\\beta + 1) \\lambda}\n$$ {#eq-gamma-times-poisson}\n\n$\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}$ 是一个常数，$k!$ 也是一个常数。\n\n有这样一个知识：对于一个 Gamma 函数 $\\Gamma(\\alpha, \\frac{1}{\\beta})$，曲线下的面积（即分布的积分）为 1。 如果乘上一个常数，比如 $2 \\cdot \\Gamma(\\alpha, \\frac{1}{\\beta})$，该分布的形状保持不变，但曲线下的面积不再是 1，而是变为该常数的值。\n\n那我们就可以理解，在 Gamma 分布中，$\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}$ 的作用就是让积分为 1。\n\n为了得到 Posterior distribution, 我们要把所有的 $\\lambda$ 代入 @eq-gamma-times-poisson，得到一串数字，最后我们要确保它们的和为 1。因此，虽然公式 @eq-gamma-times-poisson 中 \\frac{\\beta^\\alpha}{\\Gamma(\\alpha) k!} 和 Gamma 函数的不一样，但这个不重要，最后确保和为 1 的时候，它自动就变成了它该有的样子。\n\n因此我们就可以这么说：Posterior Distribution 也是一个 Gamma Distribution，其参数现在变成了 $\\alpha + k$ 和 $\\beta + 1$：\n\n$$\\lambda | k \\sim \\text{Gamma}(\\alpha + k, \\beta + 1)$$\n\n代码：\n\n我们看到这个结果和我们通过常规的 Prior 与 Likelihood 相乘所得到的结果是一样的。\n\n## 后验预测分布\n\n我们来讲一下 Posterior predictive distribution (后验预测分布)。\n\n我们看到 Posterior distribution 中，X 轴是 $\\lambda$，其含义是平均而言。比如说，$\\lambda = 5$ 代表的意思是这个球队如果比赛无数次，平均下来每场会进 5 个球。\n\n回到刚才的德国和巴西男足的叙事，我们现在如果问：\n\n>两队现在马上要进行一场比赛，请问德国对获胜的概率是多少？\n\n请注意，我们现在已经不再是问 $\\lambda$，而是问具体的一场比赛中每队的进球数，也就是 $k$。\n\n还记得我们上面提到的吗？\n\n$$\nP(k) = \\sum_{\\lambda} P(k|\\lambda) \\cdot P(\\lambda)\n$$\n\n这里 $P(\\lambda)$ 是 posterior distribution，$P(k|\\lambda)$ 是泊松分布。\n\n下面我们计算一下在下一场比赛中德国队获胜的概率。\n\n总体思路是这样：德国队随机一个 $k$，巴西队也随机选一个，德国对应的 $k$ 比巴西的大的概率。\n\n算法如下：\n\n$$\nP(k_{\\text{Germany}} > k_{\\text{Brazil}}) = \\sum_{k} P(k_{\\text{Germany}}) \\cdot P(k_{\\text{Brazil}}) \\cdot \\mathbb{I}(k_{\\text{Germany}} > k_{\\text{Brazil}})\n$$\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","output-file":"07_poisson_process.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","bibliography":["references.bib"],"theme":"cosmo","title":"泊松过程"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}